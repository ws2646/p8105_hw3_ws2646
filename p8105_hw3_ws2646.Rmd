---
title: "p8105_hw3_ws2646"
author: "Weize Sun"
date: "10/20/2021"
output: github_document
---

```{r}
library(tidyverse)
```


## Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```

```{r}
nrow(instacart)
ncol(instacart)
skimr::skim(instacart)
```

```{r, include = FALSE}
names(instacart)
```


### Short discription of the dataset

This dataset contains 1384617 rows of observations and 15 columns of variables. The structure is, each row represents one order of one product by one user from an online grocery store "Instacart".

The 15 variables are `r names(instacart)` .

### Describing some key variables

```{r}
mean(pull(instacart, reordered))
names(sort(table(pull(instacart, order_hour_of_day)), decreasing = TRUE))[1]
mean(pull(instacart, days_since_prior_order))
names(sort(table(pull(instacart, product_name)), decreasing = TRUE))[1]
```
I found there are 4 variables are important.

The first one is `reordered`, which represents if the order has been ordered or not.

The mean of `reordered` is 0.5985944, which means about 59.86% of the orders has been reordered.

The second one is `order_hour_of_day`, which shows the hour of a day when a user placed that order.

Most of the users placed order in 14, which is 2 pm.

The third one is `days_since_prior_order`, which means after how many days the users placed another order.

The mean of `days_since_prior_order` is 17.06613, which means users often placed another order after about 17 days.

The fourth one is `product_name`, which shows the name of a product.

Banana has been the most product the users ordered.

### Example of observation

```{r}
instacart[10,] %>% 
  knitr::kable()
```
I chose the 10th row from this dataset. It shows that this user ordered Spring Water from aisle "water seltzer sparkling water" at aisle 115 from department beverages. This user placed this order at 6 pm on Saturday, and it has been 30 days since this user's last order.

### About aisles

```{r}
length(unique(pull(instacart, aisle)))
names(sort(table(pull(instacart, aisle)), decreasing = TRUE)) [1]
```
There are 134 unique aisles, of which fresh vegetables was the most ordered item from.

```{r}
aisle_plot = 
  instacart %>% 
  count(aisle, name = "number_of_items") %>% 
  filter(number_of_items > 10000)  %>% 
  ggplot(aes(reorder(aisle, number_of_items), x = number_of_items)) +
  geom_point()+
  labs(
    title = "Number of items (more than 10,000 times) ordered in each aisle",
    x = "Number of items",
    y = "Aisle name",
    caption = "Data from p8105 website"
  )
aisle_plot
```

### Popular items in aisles

```{r}
three_pop_items_df = 
  instacart %>% 
  select(aisle, product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%           
  group_by(aisle, product_name) %>%
  summarize(order_times = n()) %>%
  arrange(desc(order_times)) %>%
  mutate(product_rank = min_rank(desc(order_times))) %>% 
  filter(product_rank <= 3)

knitr::kable(three_pop_items_df)
```

This is the table showing the most popular three items in aisles "packaged vegetables fruits", "baking ingredients", and "dog food care".

Under aisle "packaged vegetables fruits", Organic Baby Spinach is the most popular item, which has been ordered 9784 times; Organic Raspberries is the second popular item, which has been ordered 5546 times; Organic Blueberries is the third popular item, which has been ordered 4966 times.

Under aisle "baking ingredients", Light Brown Sugar is the most popular item, which has been ordered 499 times; Pure Baking Soda is the second popular item, which has been ordered 387 times; Pure Baking Soda is the third popular item, which has been ordered 336 times.

Under aisle "dog food care", Snack Sticks Chicken & Rice Recipe Dog Treats is the most popular item, which has been ordered 30 times; Organic Chicken & Brown Rice Recipe is the second popular item, which has been ordered 28 times; Small Dog Biscuits is the third popular item, which has been ordered 26 times.

### Mean order hour of items

```{r}
hour_df = 
  instacart %>% 
  select(order_dow, order_hour_of_day, product_name) %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>% 
  rename("Sunday" = "0", "Monday" = "1", "Tuesday" = "2", "Wednesday" = "3", "Thursday" = "4",
         "Friday" = "5", "Saturday" = "6") 

knitr::kable(hour_df)
```
This is the table showing the mean order hour of a day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. From this table, we can see that in general Pink Lady Apples is ordered earlier in a day than Coffee Ice Cream.

## Problem 2

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

### Cleaning data

```{r}
brfss_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  select(-locationabbr) %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>% 
  filter(topic == "Overall Health", response == "Poor" | response == "Fair" | response == "Good" | 
           response == "Very Good" | response == "Excellent") %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered = TRUE)) %>% 
  arrange(response)

brfss_clean
```

### Answering question

```{r}
brfss_2002 = 
  filter(brfss_clean, year == 2002) %>% 
  group_by(state) %>% 
  summarize(observed = n_distinct(county)) %>%
  filter(observed >= 7)

brfss_2010 = 
  filter(brfss_clean, year == 2010) %>% 
  group_by(state) %>% 
  summarize(observed = n_distinct(county)) %>%
  filter(observed >= 7)

brfss_2002
brfss_2010
```

In 2002, there are 6 states with equal to or more than 7 observed counties. They are CT, FL, MA, NC, NJ, and PA.

In 2010, there are 14 states with equal to or more than 7 observed counties. They are CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA.


```{r}
excellent_rsp = 
  filter(brfss_clean, response == "Excellent") %>% 
  group_by(year, state) %>% 
  select(year, state, data_value) %>% 
  summarize(mean_value = mean(data_value))

ggplot(excellent_rsp, aes(x = year, y = mean_value, color = state)) +
  geom_point(alpha = .5) +
  geom_line(alpha = .3) +
  labs(
    title = "The mean data value for response `Excellent` in different locations within each state",
    x = "Year",
    y = "Mean value",
    caption = "Data from p8105 website"
  )
```

```{r}
brfss_06_10 = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(locationabbr == "NY") %>%
  separate(locationdesc, into = c("state", "county"), sep = "-") %>%
  select(-locationabbr) %>% 
  filter(topic == "Overall Health", response == "Poor" | response == "Fair" | response == "Good" | response == "Very good" | response == "Excellent") %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered = TRUE)) %>%  #I don't know why brfss_clean doesn't work for me, so I have to do this over again.
  filter(year == 2006 | year == 2010) %>% 
  select(year, state, county, response, data_value) 

ggplot(brfss_06_10, aes(x = response, y = data_value, color = county))+
  geom_point(alpha = .5)+
  facet_grid(. ~ year)+
  labs(
    title = "Distribution of data value for responses among locations in NY",
    x = "Response (from Poor to Excellent)",
    y = "Data value",
    caption = "Data from p8105 website"
  )+
  theme(legend.position = "bottom")

ggplot(brfss_06_10, aes(x = data_value, fill = response))+
  geom_density(alpha = .5)+
  facet_grid(. ~ year)+
  labs(
    title = "Distribution of data value for responses among locations in NY",
    y = "Density for data value",
    x = "Data value",
    caption = "Data from p8105 website"
  )+
  theme(legend.position = "bottom")
```

In order to illustrate the dataset better, I made two two-panel plots: scatter plot and density plot. 

Based on the scatter plot, we can see that there is no data for Bronx and Erie county in year 2006.

Based on the density plot, we can see the density of each response much easier. Compare with 2006, people responded better in 2010.


## Problem 3

